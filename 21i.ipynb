{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 'Republican Senator Grassley launches new probe into Boeing, FAA'\n",
      "Logits: tensor([[ 0.1973, -0.1123, -0.4685,  0.4538,  0.0606, -0.5077,  0.0258,  0.0785]])\n",
      "Predicted class ID: 3\n",
      "Title: 'Republican Senator Grassley launches new probe into Boeing, FAA' is classified as 'New product launch'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = './saved_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Predefined order of labels\n",
    "categories = [\n",
    "    \"Equity Financing\", \"Debt Financing\", \"Bankruptcy/financial distress\",\n",
    "    \"New product launch\", \"Joint venture\", \"credit rating\", \n",
    "    \"financial results\", \"ESG announcement\"\n",
    "]\n",
    "\n",
    "# Function to classify titles\n",
    "def classify_titles(titles):\n",
    "    results = []\n",
    "    for title in titles:\n",
    "        # Tokenize the title\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Get the corresponding category\n",
    "        predicted_category = categories[predicted_class_id]\n",
    "        \n",
    "        # Debugging: print logits and predicted class ID\n",
    "        print(f\"Title: '{title}'\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        \n",
    "        results.append((title, predicted_category))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Titles to classify\n",
    "titles_to_classify = [\"Republican Senator Grassley launches new probe into Boeing, FAA\"]\n",
    "\n",
    "# Classify the titles\n",
    "classified_titles = classify_titles(titles_to_classify)\n",
    "\n",
    "# Print the results\n",
    "for title, category in classified_titles:\n",
    "    print(f\"Title: '{title}' is classified as '{category}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n",
      "target\n",
      "0    7\n",
      "1    7\n",
      "2    4\n",
      "3    4\n",
      "4    4\n",
      "5    4\n",
      "6    4\n",
      "7    4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3460.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 1979.61 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "  3%|â–Ž         | 4/120 [00:01<00:35,  3.22it/s]\n",
      "  3%|â–Ž         | 4/120 [00:01<00:35,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.062715768814087, 'eval_runtime': 0.0404, 'eval_samples_per_second': 197.971, 'eval_steps_per_second': 24.746, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 8/120 [00:02<00:36,  3.09it/s]\n",
      "  7%|â–‹         | 8/120 [00:02<00:36,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0294554233551025, 'eval_runtime': 0.0339, 'eval_samples_per_second': 235.925, 'eval_steps_per_second': 29.491, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 12/120 [00:03<00:34,  3.11it/s]\n",
      " 10%|â–ˆ         | 12/120 [00:03<00:34,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9954843521118164, 'eval_runtime': 0.0349, 'eval_samples_per_second': 229.183, 'eval_steps_per_second': 28.648, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 16/120 [00:05<00:33,  3.10it/s]\n",
      " 13%|â–ˆâ–Ž        | 16/120 [00:05<00:33,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9492647647857666, 'eval_runtime': 0.0369, 'eval_samples_per_second': 216.774, 'eval_steps_per_second': 27.097, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 20/120 [00:06<00:32,  3.11it/s]\n",
      " 17%|â–ˆâ–‹        | 20/120 [00:06<00:32,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8894462585449219, 'eval_runtime': 0.0354, 'eval_samples_per_second': 225.892, 'eval_steps_per_second': 28.236, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 24/120 [00:07<00:31,  3.07it/s]\n",
      " 20%|â–ˆâ–ˆ        | 24/120 [00:07<00:31,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8264739513397217, 'eval_runtime': 0.0469, 'eval_samples_per_second': 170.668, 'eval_steps_per_second': 21.334, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:09<00:38,  2.39it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 28/120 [00:09<00:38,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7595224380493164, 'eval_runtime': 0.047, 'eval_samples_per_second': 170.344, 'eval_steps_per_second': 21.293, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 32/120 [00:11<00:37,  2.36it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 32/120 [00:11<00:37,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6887985467910767, 'eval_runtime': 0.0456, 'eval_samples_per_second': 175.329, 'eval_steps_per_second': 21.916, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:13<00:35,  2.36it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 36/120 [00:13<00:35,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.621009111404419, 'eval_runtime': 0.0434, 'eval_samples_per_second': 184.363, 'eval_steps_per_second': 23.045, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:14<00:33,  2.38it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/120 [00:14<00:33,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5524249076843262, 'eval_runtime': 0.0449, 'eval_samples_per_second': 178.254, 'eval_steps_per_second': 22.282, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:16<00:32,  2.37it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:16<00:32,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4819926023483276, 'eval_runtime': 0.0489, 'eval_samples_per_second': 163.686, 'eval_steps_per_second': 20.461, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:18<00:30,  2.37it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:18<00:30,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4134215116500854, 'eval_runtime': 0.0474, 'eval_samples_per_second': 168.832, 'eval_steps_per_second': 21.104, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:20<00:28,  2.37it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [00:20<00:28,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3531981706619263, 'eval_runtime': 0.0469, 'eval_samples_per_second': 170.669, 'eval_steps_per_second': 21.334, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:21<00:26,  2.38it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 56/120 [00:21<00:26,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.293536901473999, 'eval_runtime': 0.0459, 'eval_samples_per_second': 174.378, 'eval_steps_per_second': 21.797, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:23<00:25,  2.34it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [00:23<00:25,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2437916994094849, 'eval_runtime': 0.0474, 'eval_samples_per_second': 168.825, 'eval_steps_per_second': 21.103, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:25<00:23,  2.34it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [00:25<00:23,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.200026273727417, 'eval_runtime': 0.0479, 'eval_samples_per_second': 167.111, 'eval_steps_per_second': 20.889, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:27<00:21,  2.37it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 68/120 [00:27<00:21,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1532506942749023, 'eval_runtime': 0.0474, 'eval_samples_per_second': 168.838, 'eval_steps_per_second': 21.105, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:28<00:20,  2.34it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [00:28<00:20,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1054445505142212, 'eval_runtime': 0.0479, 'eval_samples_per_second': 167.111, 'eval_steps_per_second': 20.889, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:30<00:18,  2.35it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 76/120 [00:30<00:18,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.066031813621521, 'eval_runtime': 0.047, 'eval_samples_per_second': 170.349, 'eval_steps_per_second': 21.294, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:32<00:16,  2.38it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [00:32<00:16,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0370099544525146, 'eval_runtime': 0.0465, 'eval_samples_per_second': 172.192, 'eval_steps_per_second': 21.524, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:34<00:15,  2.36it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 84/120 [00:34<00:15,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0128631591796875, 'eval_runtime': 0.0494, 'eval_samples_per_second': 162.001, 'eval_steps_per_second': 20.25, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:35<00:13,  2.37it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 88/120 [00:35<00:13,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9898372292518616, 'eval_runtime': 0.0459, 'eval_samples_per_second': 174.17, 'eval_steps_per_second': 21.771, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:37<00:11,  2.38it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 92/120 [00:37<00:11,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9684069752693176, 'eval_runtime': 0.0493, 'eval_samples_per_second': 162.411, 'eval_steps_per_second': 20.301, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:39<00:10,  2.36it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [00:39<00:10,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9491453170776367, 'eval_runtime': 0.0434, 'eval_samples_per_second': 184.343, 'eval_steps_per_second': 23.043, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:40<00:08,  2.37it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [00:41<00:08,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9318044185638428, 'eval_runtime': 0.0469, 'eval_samples_per_second': 170.668, 'eval_steps_per_second': 21.333, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:42<00:06,  2.41it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [00:42<00:06,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9166613817214966, 'eval_runtime': 0.0459, 'eval_samples_per_second': 174.375, 'eval_steps_per_second': 21.797, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:44<00:05,  2.38it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 108/120 [00:44<00:05,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9056735038757324, 'eval_runtime': 0.0429, 'eval_samples_per_second': 186.523, 'eval_steps_per_second': 23.315, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:46<00:03,  2.35it/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 112/120 [00:46<00:03,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8989063501358032, 'eval_runtime': 0.0475, 'eval_samples_per_second': 168.496, 'eval_steps_per_second': 21.062, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:47<00:01,  2.35it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 116/120 [00:47<00:01,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8950214385986328, 'eval_runtime': 0.0449, 'eval_samples_per_second': 178.25, 'eval_steps_per_second': 22.281, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:49<00:00,  2.38it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:49<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8934305906295776, 'eval_runtime': 0.0439, 'eval_samples_per_second': 182.294, 'eval_steps_per_second': 22.787, 'epoch': 30.0}\n",
      "{'train_runtime': 49.6654, 'train_samples_per_second': 18.121, 'train_steps_per_second': 2.416, 'train_loss': 1.1097442626953125, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1001.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.8934305906295776, 'eval_runtime': 0.0464, 'eval_samples_per_second': 172.45, 'eval_steps_per_second': 21.556, 'epoch': 30.0}\n",
      "Title: 'Company XYZ Signs $40 Million Loan Agreement'\n",
      "Logits: tensor([[ 0.0801,  2.5225, -0.3319, -0.3793, -0.8046, -0.6057, -0.4809, -0.9924]])\n",
      "Predicted class ID: 1\n",
      "Title: 'DEF Corp Obtains New Line of Credit'\n",
      "Logits: tensor([[-0.0238,  2.4712, -0.3280, -0.3403, -0.8235, -0.5594, -0.4576, -1.0544]])\n",
      "Predicted class ID: 1\n",
      "Title: 'XYZ Ltd Faces Bankruptcy Due to Losses'\n",
      "Logits: tensor([[-0.0292, -0.0510,  1.8050, -0.7112, -0.4422, -0.4796, -0.5167, -0.6468]])\n",
      "Predicted class ID: 2\n",
      "Title: 'Company XYZ Signs $40 Million Loan Agreement' is classified as 'Debt Financing'\n",
      "Title: 'DEF Corp Obtains New Line of Credit' is classified as 'Debt Financing'\n",
      "Title: 'XYZ Ltd Faces Bankruptcy Due to Losses' is classified as 'Bankruptcy/financial distress'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Ensure GPU is visible\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA is available\" if torch.cuda.is_available() else \"CUDA is not available\")\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'C:\\Users\\admin\\Desktop\\SQ\\try.csv')\n",
    "\n",
    "# Predefined order of labels\n",
    "categories = [\n",
    "    \"Equity Financing\", \"Debt Financing\", \"Bankruptcy/financial distress\",\n",
    "    \"New product launch\", \"Joint venture\", \"credit rating\", \n",
    "    \"financial results\", \"ESG announcement\"\n",
    "]\n",
    "\n",
    "# Map labels to integers based on predefined order\n",
    "label_to_int = {label: i for i, label in enumerate(categories)}\n",
    "int_to_label = {i: label for i, label in enumerate(categories)}\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels using the predefined order\n",
    "data['target'] = data['target'].map(label_to_int)\n",
    "\n",
    "# Check class distribution\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['target'], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Rename the target column to labels\n",
    "train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(categories))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./saved_model',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=not torch.cuda.is_available(),  # Use CUDA if available\n",
    "    report_to=[]  \n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# Optionally, save the tokenizer as well\n",
    "tokenizer.save_pretrained('./saved_model')\n",
    "\n",
    "# Function to classify new titles\n",
    "def classify_titles(titles):\n",
    "    results = []\n",
    "    for title in titles:\n",
    "        # Tokenize the title\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Get the corresponding category\n",
    "        predicted_category = int_to_label[predicted_class_id]\n",
    "        \n",
    "        # Debugging: print logits and predicted class ID\n",
    "        print(f\"Title: '{title}'\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        \n",
    "        results.append((title, predicted_category))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Classify the titles\n",
    "titles_to_classify = [\"Company XYZ Signs $40 Million Loan Agreement\", \"DEF Corp Obtains New Line of Credit\", \"XYZ Ltd Faces Bankruptcy Due to Losses\"]\n",
    "classified_titles = classify_titles(titles_to_classify)\n",
    "\n",
    "# Print the results\n",
    "for title, category in classified_titles:\n",
    "    print(f\"Title: '{title}' is classified as '{category}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n",
      "target\n",
      "3    7\n",
      "1    7\n",
      "0    4\n",
      "5    4\n",
      "4    4\n",
      "6    4\n",
      "7    4\n",
      "2    4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 6570.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 2650.43 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at MoritzLaurer/roberta-large-zeroshot-v2.0-c and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([8, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "                                              \n",
      " 10%|â–ˆ         | 4/40 [00:07<01:01,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.16719126701355, 'eval_runtime': 0.1891, 'eval_samples_per_second': 42.296, 'eval_steps_per_second': 5.287, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 20%|â–ˆâ–ˆ        | 8/40 [00:15<01:01,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.142174482345581, 'eval_runtime': 0.2555, 'eval_samples_per_second': 31.317, 'eval_steps_per_second': 3.915, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:24<00:59,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0830109119415283, 'eval_runtime': 0.2616, 'eval_samples_per_second': 30.576, 'eval_steps_per_second': 3.822, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:32<00:51,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0499062538146973, 'eval_runtime': 0.2631, 'eval_samples_per_second': 30.41, 'eval_steps_per_second': 3.801, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:41<00:42,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9899590015411377, 'eval_runtime': 0.2643, 'eval_samples_per_second': 30.266, 'eval_steps_per_second': 3.783, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:50<00:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9170575141906738, 'eval_runtime': 0.2588, 'eval_samples_per_second': 30.917, 'eval_steps_per_second': 3.865, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:59<00:25,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8418419361114502, 'eval_runtime': 0.2613, 'eval_samples_per_second': 30.612, 'eval_steps_per_second': 3.826, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [01:08<00:17,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7827578783035278, 'eval_runtime': 0.2564, 'eval_samples_per_second': 31.207, 'eval_steps_per_second': 3.901, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [01:17<00:08,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7573444843292236, 'eval_runtime': 0.269, 'eval_samples_per_second': 29.737, 'eval_steps_per_second': 3.717, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [01:26<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7503397464752197, 'eval_runtime': 0.258, 'eval_samples_per_second': 31.013, 'eval_steps_per_second': 3.877, 'epoch': 10.0}\n",
      "{'train_runtime': 86.3574, 'train_samples_per_second': 3.474, 'train_steps_per_second': 0.463, 'train_loss': 1.8576419830322266, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1002.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.7503397464752197, 'eval_runtime': 0.2576, 'eval_samples_per_second': 31.062, 'eval_steps_per_second': 3.883, 'epoch': 10.0}\n",
      "Title: 'Company XYZ Signs $40 Million Loan Agreement'\n",
      "Logits: tensor([[-0.7195,  1.2879, -0.4163,  0.2946, -0.7942, -0.8786, -0.6896, -1.1208]])\n",
      "Predicted class ID: 1\n",
      "Title: 'DEF Corp Obtains New Line of Credit'\n",
      "Logits: tensor([[-0.7813,  1.1869, -0.3781,  0.2787, -0.7756, -0.8638, -0.6422, -1.1289]])\n",
      "Predicted class ID: 1\n",
      "Title: 'XYZ Ltd Faces Bankruptcy Due to Losses'\n",
      "Logits: tensor([[ 0.8869, -0.5310, -0.7760,  0.2495,  0.1456,  0.1347, -0.4021,  0.1975]])\n",
      "Predicted class ID: 0\n",
      "Title: 'Company XYZ Signs $40 Million Loan Agreement' is classified as 'Debt Financing'\n",
      "Title: 'DEF Corp Obtains New Line of Credit' is classified as 'Debt Financing'\n",
      "Title: 'XYZ Ltd Faces Bankruptcy Due to Losses' is classified as 'Bankruptcy/financial distress'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA is available\" if torch.cuda.is_available() else \"CUDA is not available\")\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'C:\\Users\\admin\\Desktop\\SQ\\try.csv')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Check class distribution\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['target'], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/roberta-large-zeroshot-v2.0-c\")\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Rename the target column to labels\n",
    "train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/roberta-large-zeroshot-v2.0-c\", num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./saved_model',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=not torch.cuda.is_available(),  # Use CUDA if available\n",
    "    report_to=[]  \n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# Optionally, save the tokenizer as well\n",
    "tokenizer.save_pretrained('./saved_model')\n",
    "\n",
    "# Function to classify new titles\n",
    "def classify_titles(titles):\n",
    "    results = []\n",
    "    for title in titles:\n",
    "        # Tokenize the title\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Get the corresponding category\n",
    "        predicted_category = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "        \n",
    "        # Debugging: print logits and predicted class ID\n",
    "        print(f\"Title: '{title}'\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        \n",
    "        results.append((title, predicted_category))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Classify the titles\n",
    "titles_to_classify = [\"Company XYZ Signs $40 Million Loan Agreement\", \"DEF Corp Obtains New Line of Credit\", \"XYZ Ltd Faces Bankruptcy Due to Losses\"]\n",
    "classified_titles = classify_titles(titles_to_classify)\n",
    "\n",
    "# Print the results\n",
    "for title, category in classified_titles:\n",
    "    print(f\"Title: '{title}' is classified as '{category}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
