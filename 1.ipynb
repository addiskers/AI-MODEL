{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n",
      "8\n",
      "target\n",
      "3    7\n",
      "1    7\n",
      "0    4\n",
      "5    4\n",
      "4    4\n",
      "6    4\n",
      "7    4\n",
      "2    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA is available\" if torch.cuda.is_available() else \"CUDA is not available\")\n",
    "\n",
    "# Load your dataset from Excel\n",
    "data = pd.read_csv(r'C:\\Users\\admin\\Desktop\\SQ\\try.csv')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(num_labels)\n",
    "print(data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 5841.11 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 2575.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target\n",
      "5   xyz ltd declares bankruptcy amidst financial t...       0\n",
      "3         def inc. secures $30 million loan from bank       1\n",
      "9                 abc and xyz form strategic alliance       4\n",
      "30            abc corp reports record earnings for q1       7\n",
      "34        abc corp issues new shares to raise capital       3\n",
      "36              def ltd announces debt financing deal       1\n",
      "25           xyz ltd introduces next-gen tech product       5\n",
      "14            xyz corp. announces new esg initiatives       2\n",
      "35     company xyz completes successful funding round       3\n",
      "13              xyz achieves record quarterly revenue       7\n",
      "10                major agency downgrades company xyz       6\n",
      "24           tech company def launches new smartphone       5\n",
      "16                     new share issuance by def corp       3\n",
      "20       company xyz signs $40 million loan agreement       1\n",
      "22             xyz ltd faces bankruptcy due to losses       0\n",
      "17  company abc raises $50 million in series b fun...       3\n",
      "21                def corp obtains new line of credit       1\n",
      "37               company abc secures $25 million loan       1\n",
      "6         tech giant abc introduces innovative gadget       5\n",
      "2   debt financing agreement signed by abc corpora...       1\n",
      "4   company def files for bankruptcy after major r...       0\n",
      "26                 abc and def form new joint venture       4\n",
      "33    def corp implements new sustainability programs       2\n",
      "8              two leading firms announce partnership       4\n",
      "28             credit rating agency upgrades def corp       6\n",
      "0   company xyz raises $10 million in series a fun...       3\n",
      "1               abc corp announces new share issuance       3\n",
      "15    abc corp introduces new sustainability measures       2\n",
      "31      def corp announces strong year-end financials       7\n",
      "11              def corp upgraded to investment grade       6\n",
      "Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 30\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['target'], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/roberta-large-zeroshot-v2.0-c\")\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Rename the target column to labels\n",
    "train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "print(train_data)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/roberta-large-zeroshot-v2.0-c\", num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./saved_model',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# Optionally, save the tokenizer as well\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = './saved_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Function to classify new titles with confidence scores\n",
    "def classify_titles(titles):\n",
    "    results = []\n",
    "    for title in titles:\n",
    "        # Tokenize the title\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class and confidence score\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "        confidence_score = probabilities[predicted_class_id]\n",
    "        \n",
    "        # Get the corresponding category\n",
    "        predicted_category = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "        \n",
    "        # Debugging: print logits, predicted class ID, and confidence score\n",
    "        print(f\"Title: '{title}'\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        print(f\"Confidence score: {confidence_score:.4f}\")\n",
    "        \n",
    "        results.append((title, predicted_category, confidence_score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example titles to classify\n",
    "titles_to_classify = [\"Volkswagen delays ID.7 EV launch in US, Canada\"]\n",
    "classified_titles = classify_titles(titles_to_classify)\n",
    "\n",
    "# Print the results\n",
    "for title, category, confidence in classified_titles:\n",
    "    print(f\"Title: '{title}' is classified as '{category}'\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load your dataset from Excel\n",
    "data = pd.read_excel(r'C:\\Users\\admin\\Desktop\\traini\\newsdata.xlsx')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove classes with fewer than 2 samples\n",
    "min_samples = 2\n",
    "value_counts = data['target'].value_counts()\n",
    "data = data[data['target'].isin(value_counts[value_counts >= min_samples].index)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Number of classes after filtering: {num_labels}\")\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"MoritzLaurer/roberta-large-zeroshot-v2.0-c\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Cross-validation training and evaluation\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data, data['target'])):\n",
    "    print(f\"Training fold {fold + 1}\")\n",
    "    \n",
    "    train_data = data.iloc[train_index].reset_index(drop=True)\n",
    "    val_data = data.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "    \n",
    "    # Tokenize the datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # Rename the target column to labels\n",
    "    train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "    val_dataset = val_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_fold_{fold}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        use_cpu=not torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Evaluation results for fold {fold + 1}: {results}\")\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f'./results_fold_{fold}')\n",
    "    tokenizer.save_pretrained(f'./results_fold_{fold}')\n",
    "    \n",
    "    # Calculate predictions and true labels\n",
    "    predictions = np.argmax(trainer.predict(val_dataset).predictions, axis=1)\n",
    "    true_labels = val_dataset['labels'].numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    # Append metrics for this fold\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print(f\"Average accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average precision: {average_precision:.4f}\")\n",
    "print(f\"Average recall: {average_recall:.4f}\")\n",
    "print(f\"Average F1-score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Determine the best model based on average F1-score (or another metric)\n",
    "best_fold_index = np.argmax(f1_scores)\n",
    "print(f\"The best model is from fold {best_fold_index + 1}\")\n",
    "\n",
    "# Load the best model for final evaluation on the test set\n",
    "model_path = f'./results_fold_{best_fold_index}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Function to classify new titles and calculate accuracy on the test set\n",
    "def classify_and_evaluate(test_dataset):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs = {\n",
    "            'input_ids': test_dataset[i]['input_ids'].unsqueeze(0),\n",
    "            'attention_mask': test_dataset[i]['attention_mask'].unsqueeze(0)\n",
    "        }\n",
    "        labels = test_dataset[i]['labels'].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "        confidence_score = probabilities[predicted_class_id]\n",
    "\n",
    "        # Append predictions and true labels\n",
    "        all_predictions.append(predicted_class_id)\n",
    "        all_labels.append(labels.numpy()[0])\n",
    "\n",
    "        # Debugging: print logits, predicted class ID, and confidence score\n",
    "        print(f\"Sample {i}\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        print(f\"Confidence score: {confidence_score:.4f}\")\n",
    "        print(f\"True label: {labels.numpy()[0]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    return accuracy\n",
    "\n",
    "# Convert test data to Hugging Face Dataset and tokenize\n",
    "test_data = data.iloc[val_index].reset_index(drop=True)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy = classify_and_evaluate(test_dataset)\n",
    "print(f\"Accuracy on the test dataset: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes after filtering: 19\n",
      "target\n",
      "16    34\n",
      "17    32\n",
      "10    18\n",
      "18    13\n",
      "5     12\n",
      "6     11\n",
      "8      9\n",
      "1      8\n",
      "2      7\n",
      "7      6\n",
      "3      3\n",
      "0      3\n",
      "14     3\n",
      "13     3\n",
      "11     3\n",
      "12     2\n",
      "9      2\n",
      "4      2\n",
      "15     2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 18:29:52,802] A new study created in memory with name: no-name-9f197fe0-6b81-44cc-8af3-4e841768777a\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 19762.84 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 10973.29 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at MoritzLaurer/roberta-large-zeroshot-v2.0-c and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13848\\1641727154.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [19:09<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6152985095977783, 'eval_runtime': 2.9661, 'eval_samples_per_second': 11.8, 'eval_steps_per_second': 1.686, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [20:23<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5781986713409424, 'eval_runtime': 2.169, 'eval_samples_per_second': 16.136, 'eval_steps_per_second': 2.305, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [21:37<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5664045810699463, 'eval_runtime': 2.2079, 'eval_samples_per_second': 15.852, 'eval_steps_per_second': 2.265, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [22:47<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5602946281433105, 'eval_runtime': 2.1889, 'eval_samples_per_second': 15.99, 'eval_steps_per_second': 2.284, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [23:57<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5373024940490723, 'eval_runtime': 2.1843, 'eval_samples_per_second': 16.023, 'eval_steps_per_second': 2.289, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [25:12<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.519718885421753, 'eval_runtime': 2.2058, 'eval_samples_per_second': 15.867, 'eval_steps_per_second': 2.267, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [26:23<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4960179328918457, 'eval_runtime': 2.1832, 'eval_samples_per_second': 16.032, 'eval_steps_per_second': 2.29, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [27:34<05:32,  2.64s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.489193916320801, 'eval_runtime': 2.1819, 'eval_samples_per_second': 16.041, 'eval_steps_per_second': 2.292, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [09:44<00:00,  7.31s/it]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 584.4973, 'train_samples_per_second': 1.889, 'train_steps_per_second': 0.137, 'train_loss': 2.489993667602539, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 24045.11 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 8774.17 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at MoritzLaurer/roberta-large-zeroshot-v2.0-c and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13848\\1641727154.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [28:59<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6826119422912598, 'eval_runtime': 1.9393, 'eval_samples_per_second': 18.048, 'eval_steps_per_second': 2.578, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [30:13<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5644454956054688, 'eval_runtime': 1.9375, 'eval_samples_per_second': 18.065, 'eval_steps_per_second': 2.581, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [31:19<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.521848440170288, 'eval_runtime': 1.9423, 'eval_samples_per_second': 18.019, 'eval_steps_per_second': 2.574, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [32:38<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4516537189483643, 'eval_runtime': 1.9373, 'eval_samples_per_second': 18.066, 'eval_steps_per_second': 2.581, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [33:47<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.398916721343994, 'eval_runtime': 1.9571, 'eval_samples_per_second': 17.884, 'eval_steps_per_second': 2.555, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [34:56<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3602490425109863, 'eval_runtime': 1.9453, 'eval_samples_per_second': 17.992, 'eval_steps_per_second': 2.57, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [36:14<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3529090881347656, 'eval_runtime': 1.9383, 'eval_samples_per_second': 18.057, 'eval_steps_per_second': 2.58, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [37:29<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.36598801612854, 'eval_runtime': 1.9548, 'eval_samples_per_second': 17.904, 'eval_steps_per_second': 2.558, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [09:44<00:00,  7.30s/it]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 584.3901, 'train_samples_per_second': 1.889, 'train_steps_per_second': 0.137, 'train_loss': 2.422211456298828, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 21674.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 7787.01 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at MoritzLaurer/roberta-large-zeroshot-v2.0-c and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13848\\1641727154.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [38:56<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.631277084350586, 'eval_runtime': 2.4726, 'eval_samples_per_second': 14.155, 'eval_steps_per_second': 2.022, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [40:12<05:32,  2.64s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5164554119110107, 'eval_runtime': 2.4695, 'eval_samples_per_second': 14.173, 'eval_steps_per_second': 2.025, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-19 18:52:05,015] Trial 0 failed with parameters: {'learning_rate': 1.0491958148795307e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 15} because of the following error: RuntimeError('[enforce fail at inline_container.cc:595] . unexpected pos 403767616 vs 403767504').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py\", line 628, in save\n",
      "    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py\", line 862, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "RuntimeError: [enforce fail at inline_container.cc:769] . PytorchStreamWriter failed writing file data/134: file write failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13848\\1641727154.py\", line 109, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py\", line 1885, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py\", line 2311, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py\", line 2732, in _maybe_log_save_evaluate\n",
      "    self._save_checkpoint(model, trial, metrics=metrics)\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py\", line 2815, in _save_checkpoint\n",
      "    self._save_optimizer_and_scheduler(output_dir)\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py\", line 2925, in _save_optimizer_and_scheduler\n",
      "    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py\", line 627, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py\", line 475, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 403767616 vs 403767504\n",
      "[W 2024-06-19 18:52:05,035] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:595] . unexpected pos 403767616 vs 403767504",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 862\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:769] . PytorchStreamWriter failed writing file data/134: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Create a study and optimize\u001b[39;00m\n\u001b[0;32m    121\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 109\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    100\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    101\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    102\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m    106\u001b[0m )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    112\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(trainer\u001b[38;5;241m.\u001b[39mpredict(val_dataset)\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:2311\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2315\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:2732\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2729\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:2815\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[0;32m   2814\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m-> 2815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2816\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[0;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[1;34m(self, output_dir)\u001b[0m\n\u001b[0;32m   2920\u001b[0m     save_fsdp_optimizer(\n\u001b[0;32m   2921\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[0;32m   2922\u001b[0m     )\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2924\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[1;32m-> 2925\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   2929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[0;32m   2930\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\serialization.py:475\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 403767616 vs 403767504"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load your dataset from Excel\n",
    "data = pd.read_excel(r'C:\\Users\\admin\\Desktop\\traini\\newsdata.xlsx')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove classes with fewer than 2 samples\n",
    "min_samples = 2\n",
    "value_counts = data['target'].value_counts()\n",
    "data = data[data['target'].isin(value_counts[value_counts >= min_samples].index)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Number of classes after filtering: {num_labels}\")\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"MoritzLaurer/roberta-large-zeroshot-v2.0-c\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    accuracies = []\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(data, data['target'])):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        \n",
    "        train_data = data.iloc[train_index].reset_index(drop=True)\n",
    "        val_data = data.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        train_dataset = Dataset.from_pandas(train_data)\n",
    "        val_dataset = Dataset.from_pandas(val_data)\n",
    "        \n",
    "        # Tokenize the datasets\n",
    "        train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "        val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "        # Rename the target column to labels\n",
    "        train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "        val_dataset = val_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "        # Set format for PyTorch\n",
    "        train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "        # Load the model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "        # Define hyperparameters to tune\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
    "        num_train_epochs = trial.suggest_int('num_train_epochs', 2, 10)\n",
    "        per_device_train_batch_size = trial.suggest_int('per_device_train_batch_size', 4, 16)\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results_fold_{fold}',\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=0.01,\n",
    "            use_cpu=not torch.cuda.is_available(),\n",
    "            report_to=[],\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "\n",
    "        # Create a Trainer instance\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate the model\n",
    "        predictions = np.argmax(trainer.predict(val_dataset).predictions, axis=1)\n",
    "        true_labels = val_dataset['labels'].numpy()\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    return average_accuracy\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data, data['target'])):\n",
    "    print(f\"Training fold {fold + 1}\")\n",
    "    \n",
    "    train_data = data.iloc[train_index].reset_index(drop=True)\n",
    "    val_data = data.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "    \n",
    "    # Tokenize the datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # Rename the target column to labels\n",
    "    train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "    val_dataset = val_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "    # Define training arguments with the best hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_fold_{fold}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=best_params['num_train_epochs'],\n",
    "        weight_decay=0.01,\n",
    "        use_cpu=not torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Evaluation results for fold {fold + 1}: {results}\")\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f'./results_fold_{fold}')\n",
    "    tokenizer.save_pretrained(f'./results_fold_{fold}')\n",
    "    \n",
    "    # Calculate predictions and true labels\n",
    "    predictions = np.argmax(trainer.predict(val_dataset).predictions, axis=1)\n",
    "    true_labels = val_dataset['labels'].numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    # Append metrics for this fold\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print(f\"Average accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average precision: {average_precision:.4f}\")\n",
    "print(f\"Average recall: {average_recall:.4f}\")\n",
    "print(f\"Average F1-score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Determine the best model based on average F1-score (or another metric)\n",
    "best_fold_index = np.argmax(f1_scores)\n",
    "print(f\"The best model is from fold {best_fold_index + 1}\")\n",
    "\n",
    "# Load the best model for final evaluation on the test set\n",
    "model_path = f'./results_fold_{best_fold_index}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Function to classify new titles and calculate accuracy on the test set\n",
    "def classify_and_evaluate(test_dataset):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs = {\n",
    "            'input_ids': test_dataset[i]['input_ids'].unsqueeze(0),\n",
    "            'attention_mask': test_dataset[i]['attention_mask'].unsqueeze(0)\n",
    "        }\n",
    "        labels = test_dataset[i]['labels'].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "        confidence_score = probabilities[predicted_class_id]\n",
    "\n",
    "        # Append predictions and true labels\n",
    "        all_predictions.append(predicted_class_id)\n",
    "        all_labels.append(labels.numpy()[0])\n",
    "\n",
    "        # Debugging: print logits, predicted class ID, and confidence score\n",
    "        print(f\"Sample {i}\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        print(f\"Confidence score: {confidence_score:.4f}\")\n",
    "        print(f\"True label: {labels.numpy()[0]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    return accuracy\n",
    "\n",
    "# Convert test data to Hugging Face Dataset and tokenize\n",
    "test_data = data.iloc[val_index].reset_index(drop=True)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy = classify_and_evaluate(test_dataset)\n",
    "print(f\"Accuracy on the test dataset: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
