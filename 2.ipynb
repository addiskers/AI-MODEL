{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load your dataset from Excel\n",
    "data = pd.read_excel(r'C:\\Users\\admin\\Desktop\\traini\\newsdata.xlsx')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove classes with fewer than 2 samples\n",
    "min_samples = 2\n",
    "value_counts = data['target'].value_counts()\n",
    "data = data[data['target'].isin(value_counts[value_counts >= min_samples].index)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 4795.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 2697.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  target\n",
      "92   with ukraine losing ground, allies debate how ...       6\n",
      "83   hedge flow hedge funds buy chinese stocks for ...      17\n",
      "136  japan's factory activity expands for first tim...       5\n",
      "31   european commission official sees $100 bln in ...       8\n",
      "96   colorado the first state to move forward with ...      16\n",
      "..                                                 ...     ...\n",
      "134  steady, widening service price rises may spur ...      16\n",
      "156  metaâ€™s new ai council is composed entirely of ...      13\n",
      "93   weak yuan boosts the appeal of hong kong stock...       2\n",
      "29   shell sees emerging asian markets taking more ...       6\n",
      "19   explainer: what new caledonia riots mean for t...       6\n",
      "\n",
      "[138 rows x 2 columns]\n",
      "Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 138\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['target'], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Rename the target column to labels\n",
    "train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "print(train_data)\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "                                                \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:02:22<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.666346549987793, 'eval_runtime': 0.2214, 'eval_samples_per_second': 158.053, 'eval_steps_per_second': 22.579, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:02:30<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4905195236206055, 'eval_runtime': 0.3057, 'eval_samples_per_second': 114.485, 'eval_steps_per_second': 16.355, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:02:39<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.373185873031616, 'eval_runtime': 0.3005, 'eval_samples_per_second': 116.488, 'eval_steps_per_second': 16.641, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:02:48<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.225414991378784, 'eval_runtime': 0.3027, 'eval_samples_per_second': 115.615, 'eval_steps_per_second': 16.516, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:02:57<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1004581451416016, 'eval_runtime': 0.2978, 'eval_samples_per_second': 117.545, 'eval_steps_per_second': 16.792, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:03:06<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.947031021118164, 'eval_runtime': 0.3155, 'eval_samples_per_second': 110.925, 'eval_steps_per_second': 15.846, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:03:15<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8649410009384155, 'eval_runtime': 0.2988, 'eval_samples_per_second': 117.153, 'eval_steps_per_second': 16.736, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:03:24<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8271692991256714, 'eval_runtime': 0.2971, 'eval_samples_per_second': 117.801, 'eval_steps_per_second': 16.829, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:03:33<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7408959865570068, 'eval_runtime': 0.3045, 'eval_samples_per_second': 114.934, 'eval_steps_per_second': 16.419, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:03:43<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7231405973434448, 'eval_runtime': 0.3078, 'eval_samples_per_second': 113.709, 'eval_steps_per_second': 16.244, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:03:52<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6610407829284668, 'eval_runtime': 0.3003, 'eval_samples_per_second': 116.536, 'eval_steps_per_second': 16.648, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:01<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7291139364242554, 'eval_runtime': 0.2969, 'eval_samples_per_second': 117.868, 'eval_steps_per_second': 16.838, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:10<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.681612491607666, 'eval_runtime': 0.2969, 'eval_samples_per_second': 117.903, 'eval_steps_per_second': 16.843, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:19<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6539177894592285, 'eval_runtime': 0.2971, 'eval_samples_per_second': 117.823, 'eval_steps_per_second': 16.832, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:28<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7293065786361694, 'eval_runtime': 0.3022, 'eval_samples_per_second': 115.826, 'eval_steps_per_second': 16.547, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:37<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6991902589797974, 'eval_runtime': 0.289, 'eval_samples_per_second': 121.089, 'eval_steps_per_second': 17.298, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:46<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7011017799377441, 'eval_runtime': 0.3049, 'eval_samples_per_second': 114.8, 'eval_steps_per_second': 16.4, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:04:56<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.717252254486084, 'eval_runtime': 0.3054, 'eval_samples_per_second': 114.604, 'eval_steps_per_second': 16.372, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:05:05<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7530707120895386, 'eval_runtime': 0.3072, 'eval_samples_per_second': 113.928, 'eval_steps_per_second': 16.275, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:05:15<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.759432315826416, 'eval_runtime': 0.3027, 'eval_samples_per_second': 115.615, 'eval_steps_per_second': 16.516, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:05:25<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.744513988494873, 'eval_runtime': 0.4017, 'eval_samples_per_second': 87.12, 'eval_steps_per_second': 12.446, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:05:38<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7515541315078735, 'eval_runtime': 0.562, 'eval_samples_per_second': 62.283, 'eval_steps_per_second': 8.898, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:05:54<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7490975856781006, 'eval_runtime': 0.5786, 'eval_samples_per_second': 60.49, 'eval_steps_per_second': 8.641, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  6%|â–Œ         | 31/540 [1:06:07<04:15,  1.99it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7966868877410889, 'eval_runtime': 0.4266, 'eval_samples_per_second': 82.036, 'eval_steps_per_second': 11.719, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     22\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\accelerate\\accelerator.py:2134\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2134\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=not torch.cuda.is_available(),  # Use CUDA if available\n",
    "    report_to=[]  \n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./bert')\n",
    "\n",
    "# Optionally, save the tokenizer as well\n",
    "tokenizer.save_pretrained('./bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 'Turkish Rates Likely Already at Peak Even If Inflation Isnâ€™t Yet'\n",
      "Predicted class ID: 5\n",
      "Confidence score: 0.5957\n",
      "Title: 'Turkish Rates Likely Already at Peak Even If Inflation Isnâ€™t Yet' is classified as 'Industry & Economic Updates'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "# Load the saved model and tokenizer\n",
    "model_path = './bert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Function to classify new titles\n",
    "def classify_titles(titles):\n",
    "    results = []\n",
    "    for title in titles:\n",
    "        # Tokenize the title\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class and confidence score\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "        confidence_score = probabilities[predicted_class_id]\n",
    "        \n",
    "        # Get the corresponding category\n",
    "        predicted_category = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "        \n",
    "        # Debugging: print logits, predicted class ID, and confidence score\n",
    "        print(f\"Title: '{title}'\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        print(f\"Confidence score: {confidence_score:.4f}\")\n",
    "        \n",
    "        results.append((title, predicted_category, confidence_score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example titles to classify\n",
    "titles_to_classify = [\"Turkish Rates Likely Already at Peak Even If Inflation Isnâ€™t Yet\"]\n",
    "classified_titles = classify_titles(titles_to_classify)\n",
    "\n",
    "# Print the results\n",
    "for title, category, confidence in classified_titles:\n",
    "    print(f\"Title: '{title}' is classified as '{category}'\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes after filtering: 19\n",
      "target\n",
      "16    34\n",
      "17    32\n",
      "10    18\n",
      "18    13\n",
      "5     12\n",
      "6     11\n",
      "8      9\n",
      "1      8\n",
      "2      7\n",
      "7      6\n",
      "3      3\n",
      "0      3\n",
      "14     3\n",
      "13     3\n",
      "11     3\n",
      "12     2\n",
      "9      2\n",
      "4      2\n",
      "15     2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 1896.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Predicted class ID: 5\n",
      "Confidence score: 0.5957\n",
      "True label: 16\n",
      "\n",
      "Sample 1\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.9867\n",
      "True label: 17\n",
      "\n",
      "Sample 2\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.9438\n",
      "True label: 18\n",
      "\n",
      "Sample 3\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.9556\n",
      "True label: 10\n",
      "\n",
      "Sample 4\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.9241\n",
      "True label: 3\n",
      "\n",
      "Sample 5\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.7310\n",
      "True label: 18\n",
      "\n",
      "Sample 6\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.9838\n",
      "True label: 17\n",
      "\n",
      "Sample 7\n",
      "Predicted class ID: 5\n",
      "Confidence score: 0.3041\n",
      "True label: 7\n",
      "\n",
      "Sample 8\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.5122\n",
      "True label: 11\n",
      "\n",
      "Sample 9\n",
      "Predicted class ID: 5\n",
      "Confidence score: 0.6100\n",
      "True label: 5\n",
      "\n",
      "Sample 10\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.6871\n",
      "True label: 14\n",
      "\n",
      "Sample 11\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.6826\n",
      "True label: 8\n",
      "\n",
      "Sample 12\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.8716\n",
      "True label: 8\n",
      "\n",
      "Sample 13\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.4791\n",
      "True label: 10\n",
      "\n",
      "Sample 14\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.9311\n",
      "True label: 16\n",
      "\n",
      "Sample 15\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.9599\n",
      "True label: 17\n",
      "\n",
      "Sample 16\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.8594\n",
      "True label: 5\n",
      "\n",
      "Sample 17\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6408\n",
      "True label: 6\n",
      "\n",
      "Sample 18\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.9831\n",
      "True label: 16\n",
      "\n",
      "Sample 19\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.7352\n",
      "True label: 17\n",
      "\n",
      "Sample 20\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.9730\n",
      "True label: 16\n",
      "\n",
      "Sample 21\n",
      "Predicted class ID: 1\n",
      "Confidence score: 0.8502\n",
      "True label: 16\n",
      "\n",
      "Sample 22\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6457\n",
      "True label: 0\n",
      "\n",
      "Sample 23\n",
      "Predicted class ID: 2\n",
      "Confidence score: 0.7200\n",
      "True label: 2\n",
      "\n",
      "Sample 24\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.4458\n",
      "True label: 6\n",
      "\n",
      "Sample 25\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.7213\n",
      "True label: 10\n",
      "\n",
      "Sample 26\n",
      "Predicted class ID: 1\n",
      "Confidence score: 0.5932\n",
      "True label: 1\n",
      "\n",
      "Sample 27\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.7809\n",
      "True label: 16\n",
      "\n",
      "Sample 28\n",
      "Predicted class ID: 1\n",
      "Confidence score: 0.4860\n",
      "True label: 1\n",
      "\n",
      "Sample 29\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6977\n",
      "True label: 17\n",
      "\n",
      "Sample 30\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.9829\n",
      "True label: 10\n",
      "\n",
      "Sample 31\n",
      "Predicted class ID: 6\n",
      "Confidence score: 0.7867\n",
      "True label: 13\n",
      "\n",
      "Sample 32\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.7072\n",
      "True label: 17\n",
      "\n",
      "Sample 33\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.7788\n",
      "True label: 18\n",
      "\n",
      "Sample 34\n",
      "Predicted class ID: 6\n",
      "Confidence score: 0.4551\n",
      "True label: 16\n",
      "\n",
      "Accuracy on the test dataset: 0.5429\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load your dataset from Excel\n",
    "data = pd.read_excel(r'C:\\Users\\admin\\Desktop\\traini\\newsdata.xlsx')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove classes with fewer than 2 samples\n",
    "min_samples = 2\n",
    "value_counts = data['target'].value_counts()\n",
    "data = data[data['target'].isin(value_counts[value_counts >= min_samples].index)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Number of classes after filtering: {num_labels}\")\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['target'], random_state=42)\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "# Tokenize the test dataset\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Rename the target column to labels\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = './results_fold_0'  # Replace with the correct path to your saved model\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"C:\\Users\\admin\\Desktop\\traini\\bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\admin\\Desktop\\traini\\bert\")\n",
    "\n",
    "# Function to classify new titles and calculate accuracy\n",
    "def classify_and_evaluate(test_dataset):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs = {\n",
    "            'input_ids': test_dataset[i]['input_ids'].unsqueeze(0),\n",
    "            'attention_mask': test_dataset[i]['attention_mask'].unsqueeze(0)\n",
    "        }\n",
    "        labels = test_dataset[i]['labels'].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "        confidence_score = probabilities[predicted_class_id]\n",
    "\n",
    "        # Append predictions and true labels\n",
    "        all_predictions.append(predicted_class_id)\n",
    "        all_labels.append(labels.numpy()[0])\n",
    "\n",
    "        # Debugging: print logits, predicted class ID, and confidence score\n",
    "        print(f\"Sample {i}\")\n",
    "      \n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        print(f\"Confidence score: {confidence_score:.4f}\")\n",
    "        print(f\"True label: {labels.numpy()[0]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy = classify_and_evaluate(test_dataset)\n",
    "print(f\"Accuracy on the test dataset: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes after filtering: 19\n",
      "target\n",
      "16    34\n",
      "17    32\n",
      "10    18\n",
      "18    13\n",
      "5     12\n",
      "6     11\n",
      "8      9\n",
      "1      8\n",
      "2      7\n",
      "7      6\n",
      "3      3\n",
      "0      3\n",
      "14     3\n",
      "13     3\n",
      "11     3\n",
      "12     2\n",
      "9      2\n",
      "4      2\n",
      "15     2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 5417.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 2706.55 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 10%|â–ˆ         | 18/180 [00:08<01:17,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6629793643951416, 'eval_runtime': 0.3383, 'eval_samples_per_second': 103.473, 'eval_steps_per_second': 14.782, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|â–ˆâ–ˆ        | 36/180 [00:18<01:06,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5388598442077637, 'eval_runtime': 0.3265, 'eval_samples_per_second': 107.213, 'eval_steps_per_second': 15.316, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:34<00:57,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.432974338531494, 'eval_runtime': 0.3272, 'eval_samples_per_second': 106.965, 'eval_steps_per_second': 15.281, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:44<00:49,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.32399845123291, 'eval_runtime': 0.3415, 'eval_samples_per_second': 102.477, 'eval_steps_per_second': 14.64, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:54<00:40,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.224867820739746, 'eval_runtime': 0.3277, 'eval_samples_per_second': 106.819, 'eval_steps_per_second': 15.26, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:05<00:32,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.141688585281372, 'eval_runtime': 0.3332, 'eval_samples_per_second': 105.058, 'eval_steps_per_second': 15.008, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:16<00:24,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0713295936584473, 'eval_runtime': 0.33, 'eval_samples_per_second': 106.045, 'eval_steps_per_second': 15.149, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:30<00:16,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0306930541992188, 'eval_runtime': 0.3308, 'eval_samples_per_second': 105.817, 'eval_steps_per_second': 15.117, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:41<00:08,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0003602504730225, 'eval_runtime': 0.3282, 'eval_samples_per_second': 106.653, 'eval_steps_per_second': 15.236, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:51<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9905593395233154, 'eval_runtime': 0.3297, 'eval_samples_per_second': 106.172, 'eval_steps_per_second': 15.167, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:54<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 114.1219, 'train_samples_per_second': 12.092, 'train_steps_per_second': 1.577, 'train_loss': 1.9973490397135416, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for fold 1: {'eval_loss': 1.9905593395233154, 'eval_runtime': 0.2265, 'eval_samples_per_second': 154.54, 'eval_steps_per_second': 22.077, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 28.62it/s]\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 5422.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 2371.58 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 10%|â–ˆ         | 18/180 [00:08<01:13,  2.21it/s]\n",
      " 10%|â–ˆ         | 18/180 [00:08<01:13,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.737196683883667, 'eval_runtime': 0.2883, 'eval_samples_per_second': 121.422, 'eval_steps_per_second': 17.346, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 36/180 [00:18<01:06,  2.18it/s]\n",
      " 20%|â–ˆâ–ˆ        | 36/180 [00:19<01:06,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5771827697753906, 'eval_runtime': 0.2943, 'eval_samples_per_second': 118.945, 'eval_steps_per_second': 16.992, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:32<00:57,  2.20it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:32<00:57,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5126283168792725, 'eval_runtime': 0.2848, 'eval_samples_per_second': 122.904, 'eval_steps_per_second': 17.558, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:42<00:48,  2.22it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:42<00:48,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4291553497314453, 'eval_runtime': 0.2868, 'eval_samples_per_second': 122.025, 'eval_steps_per_second': 17.432, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:53<00:40,  2.20it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:53<00:40,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3571975231170654, 'eval_runtime': 0.2958, 'eval_samples_per_second': 118.319, 'eval_steps_per_second': 16.903, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:03<00:33,  2.18it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:04<00:33,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2936275005340576, 'eval_runtime': 0.2957, 'eval_samples_per_second': 118.347, 'eval_steps_per_second': 16.907, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:13<00:24,  2.20it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:14<00:24,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2478575706481934, 'eval_runtime': 0.2808, 'eval_samples_per_second': 124.644, 'eval_steps_per_second': 17.806, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:24<00:16,  2.19it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:24<00:16,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.209359645843506, 'eval_runtime': 0.2898, 'eval_samples_per_second': 120.789, 'eval_steps_per_second': 17.256, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:38<00:08,  2.18it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:38<00:08,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.192894697189331, 'eval_runtime': 0.2878, 'eval_samples_per_second': 121.632, 'eval_steps_per_second': 17.376, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:49<00:00,  2.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:49<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.187197208404541, 'eval_runtime': 0.289, 'eval_samples_per_second': 121.112, 'eval_steps_per_second': 17.302, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:54<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 114.5036, 'train_samples_per_second': 12.052, 'train_steps_per_second': 1.572, 'train_loss': 2.051239522298177, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 33.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for fold 2: {'eval_loss': 2.187197208404541, 'eval_runtime': 0.1947, 'eval_samples_per_second': 179.74, 'eval_steps_per_second': 25.677, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 33.46it/s]\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 5981.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 4383.29 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 10%|â–ˆ         | 18/180 [00:07<01:12,  2.23it/s]\n",
      " 10%|â–ˆ         | 18/180 [00:08<01:12,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7104122638702393, 'eval_runtime': 0.3735, 'eval_samples_per_second': 93.715, 'eval_steps_per_second': 13.388, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 36/180 [00:18<01:05,  2.19it/s]\n",
      " 20%|â–ˆâ–ˆ        | 36/180 [00:18<01:05,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5723876953125, 'eval_runtime': 0.3467, 'eval_samples_per_second': 100.951, 'eval_steps_per_second': 14.422, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:31<00:57,  2.19it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:31<00:57,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.499913215637207, 'eval_runtime': 0.345, 'eval_samples_per_second': 101.446, 'eval_steps_per_second': 14.492, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:41<00:48,  2.21it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:41<00:48,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4162464141845703, 'eval_runtime': 0.3476, 'eval_samples_per_second': 100.679, 'eval_steps_per_second': 14.383, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:52<00:40,  2.20it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:52<00:40,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3385276794433594, 'eval_runtime': 0.3338, 'eval_samples_per_second': 104.867, 'eval_steps_per_second': 14.981, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:09<00:32,  2.20it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:09<00:32,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.270643949508667, 'eval_runtime': 0.3427, 'eval_samples_per_second': 102.128, 'eval_steps_per_second': 14.59, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:19<00:24,  2.22it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:19<00:24,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2232697010040283, 'eval_runtime': 0.3346, 'eval_samples_per_second': 104.591, 'eval_steps_per_second': 14.942, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:33<00:16,  2.23it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:33<00:16,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1843016147613525, 'eval_runtime': 0.3289, 'eval_samples_per_second': 106.403, 'eval_steps_per_second': 15.2, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:43<00:08,  2.21it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:43<00:08,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1633288860321045, 'eval_runtime': 0.3446, 'eval_samples_per_second': 101.557, 'eval_steps_per_second': 14.508, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:56<00:00,  2.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:57<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1595945358276367, 'eval_runtime': 0.3394, 'eval_samples_per_second': 103.134, 'eval_steps_per_second': 14.733, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:58<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 118.4227, 'train_samples_per_second': 11.653, 'train_steps_per_second': 1.52, 'train_loss': 2.062643771701389, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 28.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for fold 3: {'eval_loss': 2.1595945358276367, 'eval_runtime': 0.2297, 'eval_samples_per_second': 152.401, 'eval_steps_per_second': 21.772, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 27.93it/s]\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:00<00:00, 5312.87 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:00<00:00, 2447.72 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 10%|â–ˆ         | 18/180 [00:08<01:14,  2.18it/s]\n",
      " 10%|â–ˆ         | 18/180 [00:08<01:14,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7274956703186035, 'eval_runtime': 0.2868, 'eval_samples_per_second': 118.551, 'eval_steps_per_second': 17.434, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 36/180 [00:18<01:06,  2.18it/s]\n",
      " 20%|â–ˆâ–ˆ        | 36/180 [00:18<01:06,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.559138774871826, 'eval_runtime': 0.2939, 'eval_samples_per_second': 115.676, 'eval_steps_per_second': 17.011, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:29<00:58,  2.16it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:29<00:58,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4832592010498047, 'eval_runtime': 0.289, 'eval_samples_per_second': 117.645, 'eval_steps_per_second': 17.301, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:43<00:50,  2.14it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:43<00:50,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.446662664413452, 'eval_runtime': 0.2898, 'eval_samples_per_second': 117.333, 'eval_steps_per_second': 17.255, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:53<00:41,  2.17it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:53<00:41,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3675167560577393, 'eval_runtime': 0.2863, 'eval_samples_per_second': 118.77, 'eval_steps_per_second': 17.466, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:09<00:33,  2.15it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:10<00:33,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3071846961975098, 'eval_runtime': 0.2931, 'eval_samples_per_second': 116.003, 'eval_steps_per_second': 17.059, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:20<00:24,  2.16it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:20<00:24,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3071341514587402, 'eval_runtime': 0.2922, 'eval_samples_per_second': 116.375, 'eval_steps_per_second': 17.114, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:37<00:16,  2.12it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:37<00:16,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2528059482574463, 'eval_runtime': 0.2948, 'eval_samples_per_second': 115.328, 'eval_steps_per_second': 16.96, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:47<00:08,  2.12it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [01:47<00:08,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.253525495529175, 'eval_runtime': 0.2937, 'eval_samples_per_second': 115.748, 'eval_steps_per_second': 17.022, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [02:05<00:00,  2.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [02:05<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.248444080352783, 'eval_runtime': 0.2928, 'eval_samples_per_second': 116.102, 'eval_steps_per_second': 17.074, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [02:11<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 131.6639, 'train_samples_per_second': 10.557, 'train_steps_per_second': 1.367, 'train_loss': 2.0204199896918404, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 31.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for fold 4: {'eval_loss': 2.248444080352783, 'eval_runtime': 0.2046, 'eval_samples_per_second': 166.211, 'eval_steps_per_second': 24.443, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 32.86it/s]\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:00<00:00, 5802.17 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:00<00:00, 3071.23 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 10%|â–ˆ         | 18/180 [00:07<01:14,  2.17it/s]\n",
      " 10%|â–ˆ         | 18/180 [00:08<01:14,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7480034828186035, 'eval_runtime': 0.3223, 'eval_samples_per_second': 105.499, 'eval_steps_per_second': 15.515, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 36/180 [00:20<01:07,  2.14it/s]\n",
      " 20%|â–ˆâ–ˆ        | 36/180 [00:20<01:07,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.607926368713379, 'eval_runtime': 0.3219, 'eval_samples_per_second': 105.629, 'eval_steps_per_second': 15.534, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:38<00:59,  2.13it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:39<00:59,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5312492847442627, 'eval_runtime': 0.3152, 'eval_samples_per_second': 107.871, 'eval_steps_per_second': 15.863, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:49<00:51,  2.10it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:49<00:51,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.457970142364502, 'eval_runtime': 0.3178, 'eval_samples_per_second': 106.973, 'eval_steps_per_second': 15.731, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [01:06<00:42,  2.11it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [01:07<00:42,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4033074378967285, 'eval_runtime': 0.3248, 'eval_samples_per_second': 104.677, 'eval_steps_per_second': 15.394, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:23<00:34,  2.11it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [01:23<00:34,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.358975887298584, 'eval_runtime': 0.3317, 'eval_samples_per_second': 102.499, 'eval_steps_per_second': 15.073, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:33<00:26,  2.03it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/180 [01:34<00:26,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3189873695373535, 'eval_runtime': 0.3203, 'eval_samples_per_second': 106.151, 'eval_steps_per_second': 15.61, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:51<00:16,  2.15it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 144/180 [01:52<00:16,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2961490154266357, 'eval_runtime': 0.3242, 'eval_samples_per_second': 104.875, 'eval_steps_per_second': 15.423, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [02:06<00:08,  2.13it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 162/180 [02:06<00:08,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2917842864990234, 'eval_runtime': 0.3248, 'eval_samples_per_second': 104.688, 'eval_steps_per_second': 15.395, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [02:17<00:00,  2.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [02:17<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.285047769546509, 'eval_runtime': 0.3473, 'eval_samples_per_second': 97.889, 'eval_steps_per_second': 14.395, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [02:20<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 140.0481, 'train_samples_per_second': 9.925, 'train_steps_per_second': 1.285, 'train_loss': 2.0575547960069445, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 30.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for fold 5: {'eval_loss': 2.285047769546509, 'eval_runtime': 0.218, 'eval_samples_per_second': 155.93, 'eval_steps_per_second': 22.931, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 29.93it/s]\n",
      "c:\\Users\\admin\\Desktop\\traini\\autotrain-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.4099\n",
      "Average precision: 0.3163\n",
      "Average recall: 0.4099\n",
      "Average F1-score: 0.3231\n",
      "The best model is from fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:00<00:00, 5681.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Logits: tensor([[-0.5147,  0.1452, -0.1827,  0.3754, -0.4275,  0.1474,  0.1920, -0.2378,\n",
      "         -0.0086, -0.8068,  0.6469, -0.7972, -0.3134, -0.6750, -0.5901, -0.6189,\n",
      "          1.0171,  0.4323,  0.5441]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.1382\n",
      "True label: 3\n",
      "\n",
      "Sample 1\n",
      "Logits: tensor([[-0.2758, -0.1615, -0.1410, -0.4631, -0.6451,  0.5631,  1.4634,  0.5597,\n",
      "          0.0450, -0.6723,  0.4159, -0.5304, -0.3976, -0.7369, -0.6446, -0.5435,\n",
      "          1.1181,  0.6323,  0.0744]])\n",
      "Predicted class ID: 6\n",
      "Confidence score: 0.1847\n",
      "True label: 6\n",
      "\n",
      "Sample 2\n",
      "Logits: tensor([[-0.1676, -0.1231, -0.1395, -0.3573, -0.5591,  0.5704,  1.4357,  0.4759,\n",
      "         -0.0123, -0.7218,  0.4429, -0.6238, -0.3513, -0.7556, -0.6621, -0.4805,\n",
      "          0.9892,  0.5345,  0.0711]])\n",
      "Predicted class ID: 6\n",
      "Confidence score: 0.1844\n",
      "True label: 6\n",
      "\n",
      "Sample 3\n",
      "Logits: tensor([[-0.1351, -0.2074, -0.0946, -0.4974, -0.6130,  0.6034,  1.0048,  0.8198,\n",
      "         -0.0852, -0.6555,  0.2955, -0.3564, -0.3423, -0.5509, -0.5700, -0.4185,\n",
      "          0.8372,  0.6573,  0.1238]])\n",
      "Predicted class ID: 6\n",
      "Confidence score: 0.1245\n",
      "True label: 7\n",
      "\n",
      "Sample 4\n",
      "Logits: tensor([[-0.3799, -0.0281, -0.1570, -0.2149, -0.3883,  0.0027,  0.1371, -0.1411,\n",
      "          0.2943, -0.2421, -0.1336, -0.1966, -0.3202, -0.2792, -0.3430, -0.2871,\n",
      "          0.3640,  0.5006,  0.6838]])\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.1051\n",
      "True label: 9\n",
      "\n",
      "Sample 5\n",
      "Logits: tensor([[-0.3620, -0.2957,  0.8196, -0.1199, -0.2698,  0.2126,  0.0086, -0.5581,\n",
      "          0.1232, -0.4322,  0.2991, -0.6508, -0.4633, -0.6334, -0.3754, -0.2046,\n",
      "          0.5244,  1.5716,  0.2359]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.2163\n",
      "True label: 2\n",
      "\n",
      "Sample 6\n",
      "Logits: tensor([[-0.4846, -0.0738, -0.0880, -0.0740, -0.6975,  0.3850,  0.1944, -0.0563,\n",
      "         -0.1846, -0.5204,  0.0320, -0.6896, -0.4685, -0.5683, -0.5365, -0.2517,\n",
      "          3.0254,  0.8394, -0.1131]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.5592\n",
      "True label: 16\n",
      "\n",
      "Sample 7\n",
      "Logits: tensor([[-0.6077,  0.1360, -0.2275, -0.2677, -0.9125,  0.3526,  0.1724, -0.0362,\n",
      "         -0.0564, -0.5841,  0.0755, -0.5899, -0.5297, -0.5812, -0.5463, -0.3156,\n",
      "          3.2599,  0.5116, -0.2250]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.6302\n",
      "True label: 16\n",
      "\n",
      "Sample 8\n",
      "Logits: tensor([[-0.6439,  0.1979, -0.1873, -0.2396, -0.7907,  0.2495,  0.1366, -0.0147,\n",
      "         -0.1677, -0.4186,  0.1411, -0.6590, -0.5077, -0.6369, -0.5102, -0.3150,\n",
      "          3.3047,  0.4236, -0.3319]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.6432\n",
      "True label: 16\n",
      "\n",
      "Sample 9\n",
      "Logits: tensor([[-0.4402, -0.4693, -0.1085, -0.1354, -0.5933,  0.2253,  0.2176, -0.4832,\n",
      "          0.0036, -0.7794,  0.0516, -0.8321, -0.8415, -0.5737, -0.6577, -0.3024,\n",
      "          0.4971,  3.1475,  0.6647]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6004\n",
      "True label: 17\n",
      "\n",
      "Sample 10\n",
      "Logits: tensor([[-0.5972,  0.7142, -0.2096, -0.2591, -0.8193,  0.1239, -0.0457, -0.0861,\n",
      "         -0.2047, -0.5663,  0.3953, -0.5194, -0.4008, -0.5573, -0.4610, -0.4894,\n",
      "          2.7025,  0.2752, -0.1268]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.4862\n",
      "True label: 1\n",
      "\n",
      "Sample 11\n",
      "Logits: tensor([[-0.2380, -0.5553, -0.1015, -0.0853, -0.5042,  0.2190,  0.3091, -0.6027,\n",
      "          0.0130, -0.7453,  0.0923, -0.8838, -0.8279, -0.4499, -0.4755, -0.2574,\n",
      "          0.2042,  3.1647,  0.4617]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6087\n",
      "True label: 17\n",
      "\n",
      "Sample 12\n",
      "Logits: tensor([[-0.6784,  0.1177, -0.2201, -0.2469, -0.7988,  0.2218,  0.0763, -0.0724,\n",
      "         -0.0807, -0.5705,  0.1279, -0.6597, -0.4953, -0.6648, -0.5436, -0.3392,\n",
      "          3.4103,  0.6541, -0.3066]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.6666\n",
      "True label: 16\n",
      "\n",
      "Sample 13\n",
      "Logits: tensor([[-0.0148, -0.2778, -0.2203, -0.3597, -0.1589,  0.0026, -0.1727, -0.2667,\n",
      "          0.1637, -0.5144,  0.7366, -0.1002, -0.1648, -0.4450,  0.9660, -0.2709,\n",
      "          0.7257, -0.4105, -0.1027]])\n",
      "Predicted class ID: 14\n",
      "Confidence score: 0.1315\n",
      "True label: 14\n",
      "\n",
      "Sample 14\n",
      "Logits: tensor([[-0.6135, -0.2067, -0.1811, -0.4786, -0.4273,  0.2890, -0.1409, -0.2934,\n",
      "          1.9153, -0.4846,  0.8587, -0.3705, -0.3849, -0.5453, -0.5677, -0.6130,\n",
      "          0.5850,  0.1122,  0.4221]])\n",
      "Predicted class ID: 8\n",
      "Confidence score: 0.2868\n",
      "True label: 8\n",
      "\n",
      "Sample 15\n",
      "Logits: tensor([[-0.4611, -0.1467, -0.0356, -0.3939, -0.4202,  0.1874, -0.2454, -0.4998,\n",
      "          1.4207, -0.6698,  1.0206, -0.5547, -0.5466, -0.7204, -0.7183, -0.6392,\n",
      "          0.5354,  0.7659,  0.4476]])\n",
      "Predicted class ID: 8\n",
      "Confidence score: 0.1891\n",
      "True label: 8\n",
      "\n",
      "Sample 16\n",
      "Logits: tensor([[-0.6026,  0.1560, -0.2852, -0.2289, -0.8472,  0.2940,  0.1147, -0.0071,\n",
      "         -0.1493, -0.6060,  0.1114, -0.6136, -0.4820, -0.6110, -0.5274, -0.3422,\n",
      "          3.2770,  0.6380, -0.2289]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.6330\n",
      "True label: 16\n",
      "\n",
      "Sample 17\n",
      "Logits: tensor([[-0.3886,  0.0823, -0.2455, -0.3656, -0.4584,  0.1725,  0.1725,  0.0963,\n",
      "          0.1682, -0.3244, -0.0022, -0.1586, -0.2074,  0.1828, -0.2714, -0.1427,\n",
      "          0.8470,  0.2201,  0.4041]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.1177\n",
      "True label: 13\n",
      "\n",
      "Sample 18\n",
      "Logits: tensor([[-0.1989, -0.0660, -0.2048, -0.5730,  0.0116,  0.0595, -0.3368, -0.5095,\n",
      "          0.3087, -0.9586,  2.6236, -0.3972, -0.4401, -0.7029, -0.2742, -0.6609,\n",
      "         -0.0031,  0.3551,  0.1883]])\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.4791\n",
      "True label: 10\n",
      "\n",
      "Sample 19\n",
      "Logits: tensor([[-0.5961,  0.7583, -0.1957, -0.4978, -0.6283,  0.5368,  0.0377, -0.0218,\n",
      "          0.1175, -0.7674,  1.3020, -0.5260, -0.5017, -0.6814, -0.5343, -0.5648,\n",
      "          1.0237,  0.4402,  0.3487]])\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.1652\n",
      "True label: 1\n",
      "\n",
      "Sample 20\n",
      "Logits: tensor([[-0.1613, -0.0515, -0.2557, -0.4851,  0.0389, -0.1220, -0.3105, -0.4099,\n",
      "          0.2545, -0.8465,  2.4185, -0.4426, -0.4019, -0.7507, -0.2686, -0.6818,\n",
      "         -0.0985,  0.2232,  0.0818]])\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.4360\n",
      "True label: 10\n",
      "\n",
      "Sample 21\n",
      "Logits: tensor([[-0.6248,  0.0555, -0.4575, -0.3500, -0.4963,  0.1615,  0.0299, -0.2678,\n",
      "          0.2959, -0.7891,  0.3759, -0.3354, -0.5456, -0.4287, -0.7015, -0.3435,\n",
      "          0.2546,  0.8907,  1.5502]])\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.2207\n",
      "True label: 18\n",
      "\n",
      "Sample 22\n",
      "Logits: tensor([[-0.2915, -0.1567,  0.0301, -0.4764, -0.7074,  1.5169,  0.8248,  0.4409,\n",
      "         -0.0539, -0.8054,  0.3410, -0.8104, -0.6669, -0.7022, -0.5025, -0.2027,\n",
      "          0.8107,  1.0456, -0.1487]])\n",
      "Predicted class ID: 5\n",
      "Confidence score: 0.1917\n",
      "True label: 5\n",
      "\n",
      "Sample 23\n",
      "Logits: tensor([[-0.3334, -0.4986, -0.1815, -0.1130, -0.5707,  0.2248,  0.2105, -0.4980,\n",
      "         -0.0196, -0.7900,  0.1091, -0.8370, -0.8471, -0.5183, -0.5154, -0.3607,\n",
      "          0.2112,  3.2048,  0.5681]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6215\n",
      "True label: 17\n",
      "\n",
      "Sample 24\n",
      "Logits: tensor([[-0.2498, -0.0638, -0.2821, -0.4829, -0.0568,  0.2509, -0.1562, -0.3467,\n",
      "          0.2592, -0.8850,  2.4656, -0.5714, -0.4629, -0.7322, -0.2376, -0.6310,\n",
      "         -0.0184,  0.2992,  0.0683]])\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.4399\n",
      "True label: 10\n",
      "\n",
      "Sample 25\n",
      "Logits: tensor([[-0.2731, -0.4383, -0.0384, -0.1905, -0.5942,  0.3039,  0.2059, -0.5233,\n",
      "          0.0100, -0.8385,  0.0789, -0.9002, -0.8656, -0.5587, -0.4972, -0.3480,\n",
      "          0.1837,  3.1694,  0.4950]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6133\n",
      "True label: 17\n",
      "\n",
      "Sample 26\n",
      "Logits: tensor([[-0.6149,  0.1310, -0.1658, -0.1847, -0.7882,  0.1903,  0.1040, -0.2155,\n",
      "          0.0797, -0.4540,  0.1710, -0.5204, -0.4903, -0.5581, -0.4922, -0.2883,\n",
      "          3.1398,  0.2716, -0.2329]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.6049\n",
      "True label: 16\n",
      "\n",
      "Sample 27\n",
      "Logits: tensor([[-0.3254, -0.3148,  0.0904, -0.3346, -0.4679,  0.4356,  0.1281, -0.4139,\n",
      "          0.3372, -0.9135,  0.6263, -0.9561, -0.7270, -0.7235, -0.5470, -0.4112,\n",
      "          0.5762,  2.7647,  0.3835]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.4866\n",
      "True label: 17\n",
      "\n",
      "Sample 28\n",
      "Logits: tensor([[-0.2794, -0.1358, -0.2984, -0.1757, -0.7868,  1.2551,  0.3906, -0.1415,\n",
      "          0.0717, -0.6978,  0.3460, -0.7488, -0.5227, -0.3509, -0.5073, -0.2806,\n",
      "          1.6299,  0.4653,  0.2637]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.2160\n",
      "True label: 5\n",
      "\n",
      "Sample 29\n",
      "Logits: tensor([[ 0.3439, -0.0246, -0.0974, -0.1829, -0.5032,  0.5532,  0.6580,  0.2033,\n",
      "         -0.0031, -0.6966,  0.3510, -0.7915, -0.5178, -0.4573, -0.5846, -0.3091,\n",
      "          1.0175,  1.0138,  0.3038]])\n",
      "Predicted class ID: 16\n",
      "Confidence score: 0.1235\n",
      "True label: 0\n",
      "\n",
      "Sample 30\n",
      "Logits: tensor([[-0.2939, -0.4817, -0.0858, -0.1487, -0.5499,  0.3453,  0.1904, -0.5395,\n",
      "         -0.0568, -0.7501,  0.1103, -0.8266, -0.9095, -0.5311, -0.4787, -0.3071,\n",
      "          0.4001,  3.2006,  0.4055]])\n",
      "Predicted class ID: 17\n",
      "Confidence score: 0.6170\n",
      "True label: 17\n",
      "\n",
      "Sample 31\n",
      "Logits: tensor([[-0.6525,  0.0345, -0.3423, -0.2992, -0.3433, -0.1394, -0.0060, -0.3988,\n",
      "          0.2819, -0.8804,  0.8025, -0.5204, -0.3335, -0.6787, -0.6511, -0.5618,\n",
      "          0.6318,  0.8359,  1.2208]])\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.1637\n",
      "True label: 18\n",
      "\n",
      "Sample 32\n",
      "Logits: tensor([[-0.6145, -0.0607, -0.2918, -0.4178, -0.3183, -0.2343, -0.1507, -0.2744,\n",
      "          0.4413, -0.6589,  0.5895,  0.0881, -0.3098, -0.4664, -0.4909, -0.5471,\n",
      "          0.4836,  0.6461,  0.7343]])\n",
      "Predicted class ID: 18\n",
      "Confidence score: 0.1088\n",
      "True label: 11\n",
      "\n",
      "Sample 33\n",
      "Logits: tensor([[-0.2335, -0.1603, -0.1796, -0.4379,  0.0970, -0.1208, -0.2640, -0.4139,\n",
      "          0.3372, -0.7954,  2.4960, -0.4074, -0.2676, -0.6540, -0.2781, -0.6701,\n",
      "         -0.1777,  0.3906,  0.2071]])\n",
      "Predicted class ID: 10\n",
      "Confidence score: 0.4446\n",
      "True label: 10\n",
      "\n",
      "Accuracy on the test dataset: 0.7059\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure GPU is visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load your dataset from Excel\n",
    "data = pd.read_excel(r'C:\\Users\\admin\\Desktop\\traini\\newsdata.xlsx')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove classes with fewer than 2 samples\n",
    "min_samples = 2\n",
    "value_counts = data['target'].value_counts()\n",
    "data = data[data['target'].isin(value_counts[value_counts >= min_samples].index)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['target'] = label_encoder.fit_transform(data['target'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Number of classes after filtering: {num_labels}\")\n",
    "print(data['target'].value_counts())\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Cross-validation training and evaluation\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data, data['target'])):\n",
    "    print(f\"Training fold {fold + 1}\")\n",
    "    \n",
    "    train_data = data.iloc[train_index].reset_index(drop=True)\n",
    "    val_data = data.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "    \n",
    "    # Tokenize the datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # Rename the target column to labels\n",
    "    train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "    val_dataset = val_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Load the model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_fold_{fold}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        use_cpu=not torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Evaluation results for fold {fold + 1}: {results}\")\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f'./results_fold_{fold}')\n",
    "    tokenizer.save_pretrained(f'./results_fold_{fold}')\n",
    "    \n",
    "    # Calculate predictions and true labels\n",
    "    predictions = np.argmax(trainer.predict(val_dataset).predictions, axis=1)\n",
    "    true_labels = val_dataset['labels'].numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    # Append metrics for this fold\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print(f\"Average accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average precision: {average_precision:.4f}\")\n",
    "print(f\"Average recall: {average_recall:.4f}\")\n",
    "print(f\"Average F1-score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Determine the best model based on average F1-score (or another metric)\n",
    "best_fold_index = np.argmax(f1_scores)\n",
    "print(f\"The best model is from fold {best_fold_index + 1}\")\n",
    "\n",
    "# Load the best model for final evaluation on the test set\n",
    "model_path = f'./results_fold_{best_fold_index}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Function to classify new titles and calculate accuracy on the test set\n",
    "def classify_and_evaluate(test_dataset):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs = {\n",
    "            'input_ids': test_dataset[i]['input_ids'].unsqueeze(0),\n",
    "            'attention_mask': test_dataset[i]['attention_mask'].unsqueeze(0)\n",
    "        }\n",
    "        labels = test_dataset[i]['labels'].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "        confidence_score = probabilities[predicted_class_id]\n",
    "\n",
    "        # Append predictions and true labels\n",
    "        all_predictions.append(predicted_class_id)\n",
    "        all_labels.append(labels.numpy()[0])\n",
    "\n",
    "        # Debugging: print logits, predicted class ID, and confidence score\n",
    "        print(f\"Sample {i}\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "        print(f\"Confidence score: {confidence_score:.4f}\")\n",
    "        print(f\"True label: {labels.numpy()[0]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    return accuracy\n",
    "\n",
    "# Convert test data to Hugging Face Dataset and tokenize\n",
    "test_data = data.iloc[val_index].reset_index(drop=True)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.rename_column(\"target\", \"labels\")\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy = classify_and_evaluate(test_dataset)\n",
    "print(f\"Accuracy on the test dataset: {accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
